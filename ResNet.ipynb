{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28",
      "authorship_tag": "ABX9TyO4UE9AhZqNiWleAdqmV3rn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Dipak22/Case-Studies/blob/master/ResNet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import necessary classes and functions\n",
        "\n"
      ],
      "metadata": {
        "id": "26PSZJRYf3cW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "xe1s03TZUKUL"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import Dataset , DataLoader\n",
        "from torchvision.datasets import ImageFolder\n",
        "from tqdm.notebook import tqdm\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, in_planes, planes, downsample=None, middle_conv_stride=1, residual=True):\n",
        "        \"\"\"\n",
        "        This residual block will be reused multiple times to define our model. It consists of 3 convolutional layers,\n",
        "        along with Batch Normalization and ReLU. If a downsample is needed, it will also accept a downsampling convolution\n",
        "        that will ensure our identity is equal to the output before returning.\n",
        "\n",
        "        in_planes: Expected Number of Input Planes\n",
        "        planes: Number of Planes to Map to in the Intermediate before expansion\n",
        "        downsample: Pass in a downsampling function to ensure Identity shape matches X\n",
        "        middle_conv_stride: The first block in every set of N blocks has a stride of 2 on the second convolution\n",
        "        residual: Turn the residual sum on or off\n",
        "        \"\"\"\n",
        "        super(ResidualBlock, self).__init__()\n",
        "        ### Set Convolutional Layers ###\n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, stride=1)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=middle_conv_stride, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "\n",
        "        ### Output to planes * 4 as our expansion ###\n",
        "        self.conv3 = nn.Conv2d(planes, planes*4, kernel_size=1, stride=1)\n",
        "        self.bn3 = nn.BatchNorm2d(planes*4)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "        ### This Will Exist if a Downsample Is Needed ###\n",
        "        self.downsample = downsample\n",
        "        self.residual = residual\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x # Store the identity function\n",
        "\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "\n",
        "        x = self.conv2(x)\n",
        "        x = self.bn2(x)\n",
        "        x = self.relu(x)\n",
        "\n",
        "        x = self.conv3(x)\n",
        "        x = self.bn3(x)\n",
        "        x = self.relu(x)\n",
        "\n",
        "        if self.residual:\n",
        "            if self.downsample is not None: # If our identity function has less channels or larger size we remap it\n",
        "                identity = self.downsample(identity)\n",
        "\n",
        "            x  = x + identity\n",
        "\n",
        "        return x\n",
        ""
      ],
      "metadata": {
        "id": "g1Z4S2DUgYkZ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ResNet(nn.Module):\n",
        "    def __init__(self, layer_counts, num_channels=3, num_classes=2, residual=True):\n",
        "        \"\"\"\n",
        "        ResNet Implementation (Inspired by PyTorch torchvision.models implementation)\n",
        "\n",
        "        layer_counts: Number of blocks in each set of blocks passed as a list\n",
        "        num_channels: Number of input channels to model\n",
        "        num_classes: Number of outputs for classification\n",
        "        residual: Turn on or off residual connections\n",
        "        \"\"\"\n",
        "        super(ResNet, self).__init__()\n",
        "        self.residual = residual # Store if we want residual connections\n",
        "        self.inplanes = 64 # Starting number of planes to map to from input channels\n",
        "\n",
        "        ### INITIAL SET OF CONVOLUTIONS ###\n",
        "        self.conv1 = nn.Conv2d(3, self.inplanes, kernel_size=7, stride=2, padding=3)\n",
        "        self.bn1 = nn.BatchNorm2d(self.inplanes)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "\n",
        "        ### DEFINE LAYERS ###\n",
        "        self.layer1 = self._make_layers(layer_counts[0], planes=64, stride=1)\n",
        "        self.layer2 = self._make_layers(layer_counts[1], planes=128, stride=2)\n",
        "        self.layer3 = self._make_layers(layer_counts[2], planes=256, stride=2)\n",
        "        self.layer4 = self._make_layers(layer_counts[3], planes=512, stride=2)\n",
        "\n",
        "        ### AVERAGE POOLING AND MAP TO CLASSIFIER ###\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1,1))\n",
        "        self.fc = nn.Linear(512*4, num_classes)\n",
        "\n",
        "    def _make_layers(self, num_residual_blocks, planes, stride):\n",
        "        downsample = None # Initialize downsampling as None\n",
        "        layers = nn.ModuleList() # Create a Module list to store all our convolutions\n",
        "\n",
        "        # If we have a stride of 2, or the number of planes dont match. This condition will ALWAYS BE MET only\n",
        "        #on the first block of every set of blocks\n",
        "\n",
        "        if stride != 1 or self.inplanes != planes*4:\n",
        "            ### Map to the number of wanted planes with a stride of 2 to map identity to X\n",
        "            downsample = nn.Sequential(nn.Conv2d(self.inplanes, planes*4, kernel_size=1, stride=stride),\n",
        "                                       nn.BatchNorm2d(planes*4))\n",
        "\n",
        "        ### Append this First Block with the Downsample Layer ###\n",
        "        layers.append(ResidualBlock(in_planes=self.inplanes,\n",
        "                                    planes=planes,\n",
        "                                    downsample=downsample,\n",
        "                                    middle_conv_stride=stride,\n",
        "                                    residual=self.residual))\n",
        "\n",
        "        ### Set our InPlanes to be expanded by 4 ###\n",
        "        self.inplanes = planes * 4\n",
        "\n",
        "        ### The remaining layers shouldnt have any issues so we can just append all of teh blocks on ###\n",
        "        for _ in range(num_residual_blocks - 1):\n",
        "            layers.append(\n",
        "                ResidualBlock(\n",
        "                    in_planes=self.inplanes,\n",
        "                    planes = planes,\n",
        "                    residual=self.residual\n",
        "                )\n",
        "            )\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "\n",
        "        x = self.avgpool(x)\n",
        "        x = x.reshape(x.shape[0], -1)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "### PreDefine Different ResNet Models ###\n",
        "def ResNet50(residual=True):\n",
        "    return ResNet([3,4,6,3], residual=residual)\n",
        "\n",
        "def ResNet101(residual=True):\n",
        "    return ResNet([3,4,23,3], residual=residual)\n",
        "\n",
        "def ResNet152(residual=True):\n",
        "    return ResNet([3,8,36,3], residual=residual)"
      ],
      "metadata": {
        "id": "4Yi9VaPaiOSp"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## get data\n"
      ],
      "metadata": {
        "id": "Z0aI6puQmlX1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"karakaggle/kaggle-cat-vs-dog-dataset\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L_sh9fnbmaRt",
        "outputId": "e037c971-db5a-4c71-b93f-792018bdf587"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to dataset files: /kaggle/input/kaggle-cat-vs-dog-dataset\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "PATH_TO_DATA = path +\"/kagglecatsanddogs_3367a/PetImages/\"\n",
        "\n",
        "### DEFINE TRANSFORMATIONS ###\n",
        "normalizer = transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225]) ### IMAGENET MEAN/STD ###\n",
        "train_transforms = transforms.Compose([\n",
        "                                        transforms.Resize((224,224)),\n",
        "                                        transforms.RandomHorizontalFlip(),\n",
        "                                        transforms.ToTensor(),\n",
        "                                        normalizer\n",
        "                                      ])\n",
        "\n",
        "\n",
        "dataset = ImageFolder(PATH_TO_DATA, transform=train_transforms)\n",
        "\n",
        "train_samples, test_samples = int(0.9 * len(dataset)), len(dataset) - int(0.9 * len(dataset))\n",
        "train_dataset, val_dataset = torch.utils.data.random_split(dataset, lengths=[train_samples, test_samples])\n",
        "\n",
        "def train(model, device, epochs, optimizer, loss_fn, batch_size, trainloader, valloader):\n",
        "    log_training = {\"epoch\": [],\n",
        "                    \"training_loss\": [],\n",
        "                    \"training_acc\": [],\n",
        "                    \"validation_loss\": [],\n",
        "                    \"validation_acc\": []}\n",
        "\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        print(f\"Starting Epoch {epoch}\")\n",
        "        training_losses, training_accuracies = [], []\n",
        "        validation_losses, validation_accuracies = [], []\n",
        "\n",
        "        model.train() # Turn On BatchNorm and Dropout\n",
        "        for image, label in tqdm(trainloader):\n",
        "            image, label = image.to(DEVICE), label.to(DEVICE)\n",
        "            optimizer.zero_grad()\n",
        "            out = model.forward(image)\n",
        "\n",
        "            ### CALCULATE LOSS ##\n",
        "            loss = loss_fn(out, label)\n",
        "            training_losses.append(loss.item())\n",
        "\n",
        "            ### CALCULATE ACCURACY ###\n",
        "            predictions = torch.argmax(out, axis=1)\n",
        "            accuracy = (predictions == label).sum() / len(predictions)\n",
        "            training_accuracies.append(accuracy.item())\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        model.eval() # Turn Off Batchnorm\n",
        "        for image, label in tqdm(valloader):\n",
        "            image, label = image.to(DEVICE), label.to(DEVICE)\n",
        "            with torch.no_grad():\n",
        "                out = model.forward(image)\n",
        "\n",
        "                ### CALCULATE LOSS ##\n",
        "                loss = loss_fn(out, label)\n",
        "                validation_losses.append(loss.item())\n",
        "\n",
        "                ### CALCULATE ACCURACY ###\n",
        "                predictions = torch.argmax(out, axis=1)\n",
        "                accuracy = (predictions == label).sum() / len(predictions)\n",
        "                validation_accuracies.append(accuracy.item())\n",
        "\n",
        "        training_loss_mean, training_acc_mean = np.mean(training_losses), np.mean(training_accuracies)\n",
        "        valid_loss_mean, valid_acc_mean = np.mean(validation_losses), np.mean(validation_accuracies)\n",
        "\n",
        "        log_training[\"epoch\"].append(epoch)\n",
        "        log_training[\"training_loss\"].append(training_loss_mean)\n",
        "        log_training[\"training_acc\"].append(training_acc_mean)\n",
        "        log_training[\"validation_loss\"].append(valid_loss_mean)\n",
        "        log_training[\"validation_acc\"].append(valid_acc_mean)\n",
        "\n",
        "        print(\"Training Loss:\", training_loss_mean)\n",
        "        print(\"Training Acc:\", training_acc_mean)\n",
        "        print(\"Validation Loss:\", valid_loss_mean)\n",
        "        print(\"Validation Acc:\", valid_acc_mean)\n",
        "\n",
        "    return log_training, model"
      ],
      "metadata": {
        "id": "q5yB3ZLLmmyT"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### SELECT DEVICE ###\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Training on Device {DEVICE}\")\n",
        "\n",
        "### MODEL TRAINING INPUTS ###\n",
        "epochs = 10\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "batch_size = 128\n",
        "\n",
        "### BUILD DATALOADERS ###\n",
        "trainloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
        "valloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
        "\n",
        "### ResNet101 With Residuals ###\n",
        "model = ResNet101(residual=True) # Use ResNet50 if there is Memory Constraints\n",
        "model = model.to(DEVICE)\n",
        "optimizer = optim.Adam(params=model.parameters(), lr=0.0001)\n",
        "\n",
        "print(\"Training With Residuals\")\n",
        "resnet_w_resid, w_model = train(model=model,\n",
        "                                device=DEVICE,\n",
        "                                epochs=epochs,\n",
        "                                optimizer=optimizer,\n",
        "                                loss_fn=loss_fn,\n",
        "                                batch_size=batch_size,\n",
        "                                trainloader=trainloader,\n",
        "                                valloader=valloader)\n",
        "\n",
        "\n",
        "### ResNet101 Without Residuals ###\n",
        "model = ResNet101(residual=False) # Use ResNet50 if there is Memory Constraints\n",
        "model = model.to(DEVICE)\n",
        "optimizer = optim.Adam(params=model.parameters(), lr=0.0001)\n",
        "\n",
        "print(\"Training Without Residuals\")\n",
        "resnet_wo_resid, wo_model = train(model=model,\n",
        "                                  device=DEVICE,\n",
        "                                  epochs=epochs,\n",
        "                                  optimizer=optimizer,\n",
        "                                  loss_fn=loss_fn,\n",
        "                                  batch_size=batch_size,\n",
        "                                  trainloader=trainloader,\n",
        "                                  valloader=valloader)"
      ],
      "metadata": {
        "id": "U2gaalpPmxaP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_tfOGyUfnh1D"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}