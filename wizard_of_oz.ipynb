{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "880912f1-9d42-4bae-98c9-2e0753fceefd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "7922bfdd-f021-4195-8390-6b30c2f2d482",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "21f80ad0-93b0-4dba-8e45-e266dfdbf41e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "8bc19ac8-afd8-42ee-bdeb-e6ead6424a2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "227076"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('wizard_of_oz.txt','r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "3eb52759-a770-4a3a-9985-1c49b0c8f027",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "﻿The Project Gutenberg eBook of The Wonderful Wizard of Oz\n",
      "    \n",
      "This ebook is for the use of anyone anywhere in the United States and\n",
      "most other parts of the world at no cost and with almost no restri\n"
     ]
    }
   ],
   "source": [
    "print(text[:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "f27903eb-3311-47b4-8142-c28560e0b265",
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = list(sorted(set(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1e968c09-c9cb-47db-91a2-ae0969f0ac95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89\n"
     ]
    }
   ],
   "source": [
    "print(len(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "38604df3-d029-4224-8ee4-5472d1efce32",
   "metadata": {},
   "outputs": [],
   "source": [
    "string_to_int = {ch:i for i,ch in enumerate(chars)}\n",
    "int_to_string = {i:ch for i,ch in enumerate(chars)}\n",
    "encode = lambda s : [string_to_int[ch] for ch in s]\n",
    "decode = lambda l : \"\".join(int_to_string[i] for i in l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "73ec330a-fbb6-4211-a2bc-9d5f64b3d7cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[58, 63, 70, 55, 65]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encode('dipak')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "9c2b1f1c-e190-4d90-9a6b-1ae1a2c62a62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dipak\n"
     ]
    }
   ],
   "source": [
    "print(decode(\n",
    "[58, 63, 70, 55, 65]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "5efe1e04-e2cc-435d-a060-55d7dd47bf7f",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[104], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m data \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(encode(text), dtype \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mlong)\u001b[38;5;241m.\u001b[39mto(device)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "data = torch.tensor(encode(text), dtype = torch.long).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "ee8b0ae5-8888-422d-b115-7e3593947d1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([88, 46, 62, 59,  1, 42, 72, 69, 64, 59, 57, 74,  1, 33, 75, 74, 59, 68,\n",
       "        56, 59, 72, 61,  1, 59, 28, 69, 69, 65,  1, 69, 60,  1, 46, 62, 59,  1,\n",
       "        49, 69, 68, 58, 59, 72, 60, 75, 66,  1, 49, 63, 80, 55, 72, 58,  1, 69,\n",
       "        60,  1, 41, 80,  0,  1,  1,  1,  1,  0, 46, 62, 63, 73,  1, 59, 56, 69,\n",
       "        69, 65,  1, 63, 73,  1, 60, 69, 72,  1, 74, 62, 59,  1, 75, 73, 59,  1,\n",
       "        69, 60,  1, 55, 68, 79, 69, 68, 59,  1])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "32619810-572c-4d57-ae43-aec556a1aa9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test - train split\n",
    "n =int( 0.8 * len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7eb26533-ad0e-4b12-8fbb-4cc4bc419b54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(181660, 45416)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data), len(val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6a9ade65-76ce-4686-a33f-3598ae664512",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when input is, tensor([88]) , target is 46\n",
      "when input is, tensor([88, 46]) , target is 62\n",
      "when input is, tensor([88, 46, 62]) , target is 59\n",
      "when input is, tensor([88, 46, 62, 59]) , target is 1\n",
      "when input is, tensor([88, 46, 62, 59,  1]) , target is 42\n",
      "when input is, tensor([88, 46, 62, 59,  1, 42]) , target is 72\n",
      "when input is, tensor([88, 46, 62, 59,  1, 42, 72]) , target is 69\n",
      "when input is, tensor([88, 46, 62, 59,  1, 42, 72, 69]) , target is 64\n"
     ]
    }
   ],
   "source": [
    "#create datas set\n",
    "block_size = 8\n",
    "\n",
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size+1]\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print(f\"when input is, {context} , target is {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "109d6433-5a1c-4a02-b113-e35d9bf0a917",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters and device agnostic code\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "block_size = 8\n",
    "batch_size = 4\n",
    "n_embd = 384\n",
    "vocab_size = 80\n",
    "learning_rate = 3e-4\n",
    "max_iters = 10000\n",
    "eval_iters = 250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cb8411d3-9fa7-47b4-9621-18bfa15a31f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-62,  96, -52,  74, -55,  16],\n",
       "        [ 43,  91,  97,  13, -96, -38],\n",
       "        [-47,  58,  71, -77,  -1,  -5],\n",
       "        [ 79,  73, -70, -25,  62,  56],\n",
       "        [ 92,  77, -36,  43, -81,  81],\n",
       "        [ 62, -22, -17,  56, -64,  64]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "randint = torch.randint(-100,100,(6,6))\n",
    "randint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "252e4d25-ea51-4d1a-85f1-3a250881aef7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 6])\n",
      "tensor([[-2.6350,  0.5428, -0.6891, -1.0844, -0.5190,  0.9346],\n",
      "        [-0.4147, -0.4167, -0.3089, -0.0156,  0.4016,  0.4794],\n",
      "        [ 0.8420, -1.1484, -0.0584,  1.5400, -0.4316, -1.4974],\n",
      "        [-1.4285,  2.2053,  0.1912,  1.3495,  1.4151, -1.3279]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#define embeddings\n",
    "from torch import nn\n",
    "vocab_size = 89\n",
    "embedding_dim = 6\n",
    "embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "#create input indices\n",
    "input_indices = torch.LongTensor([1,5,3,2])\n",
    "\n",
    "embedding_output = embedding(input_indices)\n",
    "\n",
    "print(embedding_output.shape)\n",
    "print(embedding_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "f7b95954-df69-41d6-a164-7c02896977c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create dataset\n",
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    #x,y = x.to(device), y.to(device)\n",
    "    return x,y\n",
    "\n",
    "x,y = get_batch('train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8ed6943e-7462-46d8-a224-1f6950cbf41b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[55,  1, 56, 63, 74, 10, 85,  1],\n",
       "         [59, 10,  1, 55, 68, 58,  0, 74],\n",
       "         [69, 72, 65, 10,  1, 77, 62, 69],\n",
       "         [ 1, 74, 62, 59,  1, 72, 69, 55]], device='cuda:0'),\n",
       " tensor([[ 1, 56, 63, 74, 10, 85,  1, 73],\n",
       "         [10,  1, 55, 68, 58,  0, 74, 62],\n",
       "         [72, 65, 10,  1, 77, 62, 69,  1],\n",
       "         [74, 62, 59,  1, 72, 69, 55, 58]], device='cuda:0'))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d060599b-7345-41f0-999b-08bcc5ce35b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a5139315-8fcb-4b9c-b292-1ef0ca241d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self,vocab_size):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "    def forward(self, index, targets=None):\n",
    "        logits = self.token_embedding_table(index)\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B,T,C = logits.shape\n",
    "            logits = logits.view(B*T,C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        return logits, loss\n",
    "    def generate(self, index, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            logits, _ = self.forward(index)\n",
    "            # we only care about the prediction for the last token\n",
    "            # we ignore the rest\n",
    "            logits = logits[:,-1,:]\n",
    "            probs = F.softmax(logits, dim = -1)\n",
    "            index_next = torch.multinomial(probs, num_samples =1)\n",
    "            index = torch.cat((index, index_next), dim =1)\n",
    "        return index\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "41816c1f-945d-4bfa-96ab-810cec40c36c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " mw/z;’Vh;t.dF;d mYjlkx(W$X—S;N Cp1VIV:3VgY%O,oWkN#9•oSXh’useip$H“oxh—!C?R*d[&[7B‘r﻿JpXUN 3tv—[zB4!%[w3—wQuo*S;“*wom,*.R]3B’wkcUaiO“0sq4\n",
      ":Y#2uJzJHO,eXh;LPNhO#thEPw62]D9Q/%jj;“pj]qM/PHIjn0E—X-GFG‘BwSHoM4IoG?b0&pF\n",
      "nl K$fEZ2Uv4Jp2UIofSZ!J,F-dB5qbX•2﻿qyQWyd/kl2Ie\n",
      " m4—mQOrVZ?pMr2/,?d1[Cpy”r]—k07HAB2XliNtHXlMDl”)0:FK.B’lO#E7“q—:FS™g,RZI6]#VoDjnB,f?d&cY,cQc4a-jnvmXHo7LDnvsxIYRk—A3;2Q0Q;c﻿-KJ1G1•&Oh91N%i1G﻿QynF;#PHu2h—y—2A?2Udy1g3‘XVPTVuiTlv—,AQX“ZJg3lpj]fX0HkXWaZ0Q?tEI6*w2EgxI6*/PGspsx1HrX﻿A7—\n",
      "woxglo(:y\n"
     ]
    }
   ],
   "source": [
    "model = BigramLanguageModel(vocab_size=89).to(device)\n",
    "context = torch.ones((1,1), dtype = torch.long, device = device)\n",
    "generated_chars = decode(model.generate(context, max_new_tokens = 500)[0].tolist())\n",
    "print(generated_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "daa8a80a-0768-4049-aee9-bf339132d6d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training loop\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr = learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "717ad212-0c7b-4419-a0c3-8a69159b46ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.141587972640991\n"
     ]
    }
   ],
   "source": [
    "for _ in range(10000):\n",
    "    xb,yb = get_batch('train')\n",
    "    logits, loss = model.forward(xb,yb)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "caaadf31-4f70-4c34-8933-e0f481bd06f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " X3#?”﻿‘[I6v﻿(7ZLi1G5?)iWkle obe cUUSl“!Sq0‘pl hklyHrXF-™6Zp\n",
      "(XwaO)jathy*/•n:ZLYgemuNZa itha acsH—TyUd TVYzJztaZ/zB*w/62QS,Mj’”CQy k‘x—PPPIm8K--in.d womBkagIYB2Qqp﻿“$KR!﻿R&bt—q﻿TO”xwa,Q?ithed:minh™5R\n",
      "kn oyC7‘/kl8.$5EathQUa•Y%Y4X﻿ucuzUacQ#?q41—!Wj6Im﻿BLifY﻿7XhkabilidAs wu;rJj0q med IfPYdy7‘NLqb—™hJ•$doful,HPDA0pSigl oDO!?!Sbax4pERCAN;“T”Xjal.gFKfz$gS[2ut ader[Ohiel5m2pFSuS‘wSfz[thO‘l%wanFX3F”ri1,grv$8ia•oxB)MW7mt‘4—in d]B,f afF]R™ghacrowll-S#PGmH:iet[#i*!m[2Uwed&x$pv36MFm9gy”P5gX“p?P1OhrYR/1G.99O&\n"
     ]
    }
   ],
   "source": [
    "context = torch.ones((1,1), dtype = torch.long, device = device)\n",
    "generated_chars = decode(model.generate(context, max_new_tokens = 500)[0].tolist())\n",
    "print(generated_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "9b8e51cd-e301-4a98-b1ea-681b0b5ac0f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters and device agnostic code\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "block_size = 8\n",
    "batch_size = 4\n",
    "n_embd = 384\n",
    "vocab_size = 100\n",
    "learning_rate = 3e-4\n",
    "max_iters = 10000\n",
    "eval_iters = 250\n",
    "n_layer =4\n",
    "n_head = 4\n",
    "dropout = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "a885de1d-68c5-4590-be50-4a1f8944c1ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias = False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias = False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias = False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x) #(B,T,hs)\n",
    "        q = self.query(x) #(B,T, hs)\n",
    "        wei = q @ k.transpose(-2, -1) * k.shape[-1]**-0.5 #(B,T,T)\n",
    "        wei = wei.masked_fill(self.tril[:T,:T]==0, torch.finfo(wei.dtype).min)\n",
    "        wei = F.softmax(wei, dim = -1)\n",
    "        wei = self.dropout(wei)\n",
    "        v = self.value(x) #(B, T, hs)\n",
    "        out = wei @ v #(B, T, hs)\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(head_size * num_heads, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim = -1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4* n_embd, n_embd),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        super().__init__()\n",
    "        head_size = n_embd//n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedForward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.sa(x)\n",
    "        x = self.ln1(x + y)\n",
    "        y = self.ffwd(x)\n",
    "        x = self.ln2(x + y)\n",
    "        return x\n",
    "\n",
    "class GPTLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.positional_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "\n",
    "        self.ln_f = nn.LayerNorm(n_embd)\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "        self.apply(self.__init_weights)\n",
    "\n",
    "    def __init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean = 0.0, std =0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean = 0.0, std = 0.02)\n",
    "    def forward(self, index, targets = None):\n",
    "        B,T = index.shape\n",
    "        tok_emb = self.token_embedding_table(index)\n",
    "       # pos_emb = self.positional_embedding_table(torch.arange(T, device = device))\n",
    "       # x = tok_emb + pos_emb\n",
    "        x = self.blocks(x)\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.lm_head(x)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T,C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        return logits, loss\n",
    "        \n",
    "    def generate(self, index, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            logits, _ = self.forward(index)\n",
    "            # we only care about the prediction for the last token\n",
    "            # we ignore the rest\n",
    "            logits = logits[:,-1,:]\n",
    "            probs = F.softmax(logits, dim = -1)\n",
    "            index_next = torch.multinomial(probs, num_samples =1)\n",
    "            index = torch.cat((index, index_next), dim =1)\n",
    "        return index\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "9465fc13-9c89-48ef-acca-86edb0885a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "d2f2ee06-f816-41af-a7fa-297078e33c02",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[89], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m GPTLanguageModel(vocab_size)\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\dasd\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1343\u001b[0m, in \u001b[0;36mModule.to\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1340\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1341\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m-> 1343\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_apply(convert)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\dasd\\Lib\\site-packages\\torch\\nn\\modules\\module.py:903\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    901\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[0;32m    902\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m--> 903\u001b[0m         module\u001b[38;5;241m.\u001b[39m_apply(fn)\n\u001b[0;32m    905\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    906\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    907\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    908\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    913\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    914\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\dasd\\Lib\\site-packages\\torch\\nn\\modules\\module.py:930\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    926\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[0;32m    927\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[0;32m    928\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[0;32m    929\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m--> 930\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m fn(param)\n\u001b[0;32m    931\u001b[0m p_should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[0;32m    933\u001b[0m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\dasd\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1329\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m   1322\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[0;32m   1323\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(\n\u001b[0;32m   1324\u001b[0m             device,\n\u001b[0;32m   1325\u001b[0m             dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1326\u001b[0m             non_blocking,\n\u001b[0;32m   1327\u001b[0m             memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format,\n\u001b[0;32m   1328\u001b[0m         )\n\u001b[1;32m-> 1329\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(\n\u001b[0;32m   1330\u001b[0m         device,\n\u001b[0;32m   1331\u001b[0m         dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1332\u001b[0m         non_blocking,\n\u001b[0;32m   1333\u001b[0m     )\n\u001b[0;32m   1334\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1335\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot copy out of meta tensor; no data!\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "model = GPTLanguageModel(vocab_size).to(device)\n",
    "#optimizer = torch.optim.AdamW(model.parameters(), lr = learning_rate)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "4fa5ee0c-df03-424e-988a-e3682b414384",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[77], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m      2\u001b[0m     xb,yb \u001b[38;5;241m=\u001b[39m get_batch(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m     logits, loss \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mforward(xb,yb)\n\u001b[0;32m      4\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m      5\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "Cell \u001b[1;32mIn[75], line 85\u001b[0m, in \u001b[0;36mGPTLanguageModel.forward\u001b[1;34m(self, index, targets)\u001b[0m\n\u001b[0;32m     83\u001b[0m B,T \u001b[38;5;241m=\u001b[39m index\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m     84\u001b[0m tok_emb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoken_embedding_table(index)\n\u001b[1;32m---> 85\u001b[0m pos_emb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpositional_embedding_table(torch\u001b[38;5;241m.\u001b[39marange(T, device \u001b[38;5;241m=\u001b[39m device))\n\u001b[0;32m     86\u001b[0m x \u001b[38;5;241m=\u001b[39m tok_emb \u001b[38;5;241m+\u001b[39m pos_emb\n\u001b[0;32m     87\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks(x)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "for _ in range(1):\n",
    "    xb,yb = get_batch('train')\n",
    "    logits, loss = model.forward(xb,yb)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "61f2e134-3685-4a8e-9aeb-3f3e95df7d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create dataset\n",
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    #x,y = x.to(device), y.to(device)\n",
    "    return x,y\n",
    "\n",
    "x,y = get_batch('train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "45831eff-ad12-4e8a-812a-a9331e81f723",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'torch.dtype' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[59], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mtype\u001b[39m(torch\u001b[38;5;241m.\u001b[39mfloat32(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-inf\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "\u001b[1;31mTypeError\u001b[0m: 'torch.dtype' object is not callable"
     ]
    }
   ],
   "source": [
    "type(torch.float32('-inf'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42269765-332d-40ef-9c51-9c4e3e26cabf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
