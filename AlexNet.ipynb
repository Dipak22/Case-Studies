{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNJ9c0hAcDYKwvg5FT6NiiS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Dipak22/Case-Studies/blob/master/AlexNet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create AlexNet from **scratch**\n",
        "\n"
      ],
      "metadata": {
        "id": "VFhFSOfOHKdk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import necessary classes and functions"
      ],
      "metadata": {
        "id": "U7ks42CaJPnc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "iDKJhLnWHBHd"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision.datasets import ImageFolder\n",
        "from tqdm.notebook import tqdm\n",
        "import numpy as np\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class AlexNet(nn.Module):\n",
        "  def __init__(self, classes=2, dropout_p = 0.5):\n",
        "    super().__init__()\n",
        "    self.classes = classes\n",
        "\n",
        "    self.feature_extractor = nn.Sequential(\n",
        "        nn.Conv2d(in_channels=3, out_channels=64, kernel_size=11, stride=4, padding =2),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "        nn.BatchNorm2d(num_features=64),\n",
        "\n",
        "        nn.Conv2d(in_channels=64, out_channels=192, kernel_size=5, stride=1, padding =2),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "        nn.BatchNorm2d(num_features=192),\n",
        "\n",
        "        nn.Conv2d(in_channels=192, out_channels=384, kernel_size=3, stride=1, padding =1),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "        nn.BatchNorm2d(num_features=384),\n",
        "\n",
        "        nn.Conv2d(in_channels=384, out_channels=256, kernel_size=3, stride=1, padding =1),\n",
        "        nn.ReLU(),\n",
        "        nn.BatchNorm2d(num_features=256),\n",
        "\n",
        "        nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, stride=1, padding =1),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "        nn.BatchNorm2d(num_features=256),\n",
        "    )\n",
        "\n",
        "    self.avgpool = nn.AdaptiveAvgPool2d((6,6))\n",
        "\n",
        "    self.head = nn.Sequential(\n",
        "        nn.Dropout(dropout_p),\n",
        "        nn.Linear(256*6*6, 4096),\n",
        "        nn.ReLU(),\n",
        "        nn.Dropout(dropout_p),\n",
        "        nn.Linear(4096,4096),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(4096,classes)\n",
        "    )\n",
        "  def forward(self,x):\n",
        "    batch_size = x.shape[0]\n",
        "\n",
        "    x = self.feature_extractor(x)\n",
        "    #print(\"1st shape\", x.shape)\n",
        "    x = self.avgpool(x)\n",
        "    #print(\"2nd shape\", x.shape)\n",
        "    x = x.reshape(batch_size, -1)\n",
        "    #print(\"3rd shape\", x.shape)\n",
        "    x = self.head(x)\n",
        "    #print(\"4th shape\", x.shape)\n",
        "    return x\n"
      ],
      "metadata": {
        "id": "v937K7NcJ5zd"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### get dogsvscat data"
      ],
      "metadata": {
        "id": "ZbiNNpN7MbQz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"karakaggle/kaggle-cat-vs-dog-dataset\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LDtvhpheKyeD",
        "outputId": "c5a44eb9-7dac-456c-b63a-f84c3dd536e9"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/karakaggle/kaggle-cat-vs-dog-dataset?dataset_version_number=1...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 787M/787M [00:07<00:00, 109MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to dataset files: /root/.cache/kagglehub/datasets/karakaggle/kaggle-cat-vs-dog-dataset/versions/1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "PATH_TO_DATA = path +\"/kagglecatsanddogs_3367a/PetImages/\"\n",
        "\n",
        "##Define transformations\n",
        "normalizer = transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])\n",
        "train_transforms = transforms.Compose([\n",
        "    transforms.Resize((224,224)),\n",
        "    transforms.RandomHorizontalFlip(0.5),\n",
        "    transforms.ToTensor(),\n",
        "    normalizer\n",
        "])\n",
        "\n",
        "dataset = ImageFolder(PATH_TO_DATA, transform=train_transforms)\n",
        "\n",
        "train_samples, test_samples = int(0.9 * len(dataset)), len(dataset) - int(0.9 * len(dataset))\n",
        "\n",
        "train_dataset, test_dataset = torch.utils.data.random_split(dataset=dataset, lengths=[train_samples, test_samples])"
      ],
      "metadata": {
        "id": "_TSqhmoRMg6K"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define hyperparameters and dataloaders"
      ],
      "metadata": {
        "id": "Upah0aFtOJF4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = AlexNet()\n",
        "model = model.to(DEVICE)\n",
        "\n",
        "epochs = 5\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = 0.001)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "batch_size = 128\n",
        "\n",
        "trainloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
        "valloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4)"
      ],
      "metadata": {
        "id": "LccDe4B9OE6h"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### define train method"
      ],
      "metadata": {
        "id": "nZEW1139PLUS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, device, epochs, optimizer, loss_fn, batch_size, trainloader, valloader):\n",
        "    log_training = {\"epoch\": [],\n",
        "                    \"training_loss\": [],\n",
        "                    \"training_acc\": [],\n",
        "                    \"validation_loss\": [],\n",
        "                    \"validation_acc\": []}\n",
        "\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        print(f\"Starting Epoch {epoch}\")\n",
        "        training_losses, training_accuracies = [], []\n",
        "        validation_losses, validation_accuracies = [], []\n",
        "\n",
        "        model.train() # Turn On BatchNorm and Dropout\n",
        "        for image, label in tqdm(trainloader):\n",
        "            image, label = image.to(DEVICE), label.to(DEVICE)\n",
        "            optimizer.zero_grad()\n",
        "            out = model.forward(image)\n",
        "\n",
        "            ### CALCULATE LOSS ##\n",
        "            loss = loss_fn(out, label)\n",
        "            training_losses.append(loss.item())\n",
        "\n",
        "            ### CALCULATE ACCURACY ###\n",
        "            predictions = torch.argmax(out, axis=1)\n",
        "            accuracy = (predictions == label).sum() / len(predictions)\n",
        "            training_accuracies.append(accuracy.item())\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        model.eval() # Turn Off Batchnorm\n",
        "        for image, label in tqdm(valloader):\n",
        "            image, label = image.to(DEVICE), label.to(DEVICE)\n",
        "            with torch.no_grad():\n",
        "                out = model.forward(image)\n",
        "\n",
        "                ### CALCULATE LOSS ##\n",
        "                loss = loss_fn(out, label)\n",
        "                validation_losses.append(loss.item())\n",
        "\n",
        "                ### CALCULATE ACCURACY ###\n",
        "                predictions = torch.argmax(out, axis=1)\n",
        "                accuracy = (predictions == label).sum() / len(predictions)\n",
        "                validation_accuracies.append(accuracy.item())\n",
        "\n",
        "        training_loss_mean, training_acc_mean = np.mean(training_losses), np.mean(training_accuracies)\n",
        "        valid_loss_mean, valid_acc_mean = np.mean(validation_losses), np.mean(validation_accuracies)\n",
        "\n",
        "        log_training[\"epoch\"].append(epoch)\n",
        "        log_training[\"training_loss\"].append(training_loss_mean)\n",
        "        log_training[\"training_acc\"].append(training_acc_mean)\n",
        "        log_training[\"validation_loss\"].append(valid_loss_mean)\n",
        "        log_training[\"validation_acc\"].append(valid_acc_mean)\n",
        "\n",
        "        print(\"Training Loss:\", training_loss_mean)\n",
        "        print(\"Training Acc:\", training_acc_mean)\n",
        "        print(\"Validation Loss:\", valid_loss_mean)\n",
        "        print(\"Validation Acc:\", valid_acc_mean)\n",
        "\n",
        "    return log_training, model\n",
        "\n",
        "\n",
        "random_init_logs, model = train(model=model,\n",
        "                                device=DEVICE,\n",
        "                                epochs=epochs,\n",
        "                                optimizer=optimizer,\n",
        "                                loss_fn=loss_fn,\n",
        "                                batch_size=batch_size,\n",
        "                                trainloader=trainloader,\n",
        "                                valloader=valloader)"
      ],
      "metadata": {
        "id": "GEn8nnW1Oixb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}