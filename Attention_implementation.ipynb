{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN1f7meWAo/4aFKxkF5nlDG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Dipak22/Case-Studies/blob/master/Attention_implementation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LW5xx6LGObXM",
        "outputId": "d3732ced-ce90-4a51-ee03-70a075b4d6e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X shape:  torch.Size([4, 64, 128])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "batch_size = 4\n",
        "seq_len = 64\n",
        "embed_dim = 128\n",
        "\n",
        "x = torch.randn(batch_size, seq_len, embed_dim)\n",
        "print(\"X shape: \", x.shape)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# temporary implemetation\n",
        "\n",
        "similarity = x @ x.transpose(1,2)\n",
        "print(\"similarilty shape: \", similarity.shape)\n",
        "print(\"Pre normalized variance:\", similarity.var())\n",
        "similarity_norm = similarity/(embed_dim**0.5)\n",
        "print(\"Normalized variance: \", similarity_norm.var())\n",
        "\n",
        "attn = similarity_norm.softmax(dim=-1)\n",
        "\n",
        "context_vectors = attn@x\n",
        "print(\"output shape: \", context_vectors.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W_hbm5PURvkp",
        "outputId": "05c3cf4e-4997-406f-e99e-e74006cd37d7"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "similarilty shape:  torch.Size([4, 64, 64])\n",
            "Pre normalized variance: tensor(380.4652)\n",
            "Normalized variance:  tensor(2.9724)\n",
            "output shape:  torch.Size([4, 64, 128])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Self Attention\n",
        "class Attention(nn.Module):\n",
        "  def __init__(self, embed_dim):\n",
        "    super().__init__()\n",
        "\n",
        "    self.embed_dim = embed_dim\n",
        "    self.query = nn.Linear(self.embed_dim, self.embed_dim)\n",
        "    self.key = nn.Linear(self.embed_dim, self.embed_dim)\n",
        "    self.value = nn.Linear(self.embed_dim, self.embed_dim)\n",
        "  def forward(self,x):\n",
        "    q = self.query(x)\n",
        "    k = self.key(x)\n",
        "    v = self.value(x)\n",
        "    similarity = (q @ k.transpose(1,2))/(self.embed_dim**0.5)\n",
        "    attention = similarity.softmax(dim = -1)\n",
        "    output = attention @ v\n",
        "    return output\n",
        "\n",
        "attention = Attention(embed_dim=128)\n",
        "output = attention(x)\n",
        "print(output.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LtdGfLTKSqvg",
        "outputId": "61b8632f-1cbc-445c-eb70-52ccebc083b8"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([4, 64, 128])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Multihead Attention\n",
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self, embed_dim, n_heads, attn_p =0, proj_p=0):\n",
        "    super().__init__()\n",
        "    self.embed_dim = embed_dim\n",
        "    self.n_heads = n_heads\n",
        "    self.head_dim = self.embed_dim//self.n_heads\n",
        "    self.query = nn.Linear(self.embed_dim, self.embed_dim)\n",
        "    self.key = nn.Linear(self.embed_dim, self.embed_dim)\n",
        "    self.value = nn.Linear(self.embed_dim, self.embed_dim)\n",
        "    self.attn_drop = nn.Dropout(attn_p)\n",
        "    self.proj = nn.Linear(embed_dim, embed_dim)\n",
        "    self.proj_drop = nn.Dropout(proj_p)\n",
        "  def forward(self, x):\n",
        "    batch_size, seq_len, _ = x.shape\n",
        "    q = self.query(x).reshape(batch_size,seq_len,self.n_heads,self.head_dim).transpose(1,2).contiguous()\n",
        "    k = self.key(x).reshape(batch_size,seq_len,self.n_heads,self.head_dim).transpose(1,2).contiguous()\n",
        "    v = self.value(x).reshape(batch_size,seq_len,self.n_heads,self.head_dim).transpose(1,2).contiguous()\n",
        "\n",
        "\n",
        "    #calculate attention\n",
        "    similarity = (q @ k.transpose(-2,-1))/self.head_dim**0.5\n",
        "    attention = similarity.softmax(dim = -1)\n",
        "    attention = self.attn_drop(attention)\n",
        "    x = attention@v\n",
        "    x =x.transpose(1,2).reshape(batch_size, seq_len, self.embed_dim)\n",
        "    x = self.proj(x)\n",
        "    x = self.proj_drop(x)\n",
        "    return x\n",
        "\n",
        "model = MultiHeadAttention(64, 2)\n",
        "x = torch.rand(4,10,64)\n",
        "output = model(x)\n",
        "print(\"input shape: \", x.shape)\n",
        "print(\"output shape: \", output.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zX0wEpMCUrQE",
        "outputId": "98c9cd38-4830-4ea5-b4de-5e18ac55c5d9"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input shape:  torch.Size([4, 10, 64])\n",
            "output shape:  torch.Size([4, 10, 64])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Attention mask\n",
        "rand_attn = torch.rand(1,6,6)\n",
        "attention_mask = torch.tensor([1,1,1,1,0,0]).unsqueeze(0).unsqueeze(1).bool()\n",
        "rand_attn.masked_fill_(~attention_mask, float(\"-inf\"))\n",
        "rand_attn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VrdsNn2OS_Q0",
        "outputId": "feb016c7-75b5-45e9-8f1a-531e54f05ea1"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[0.6870, 0.6528, 0.0015, 0.9785,   -inf,   -inf],\n",
              "         [0.6008, 0.3150, 0.7785, 0.2160,   -inf,   -inf],\n",
              "         [0.5098, 0.6347, 0.9880, 0.2996,   -inf,   -inf],\n",
              "         [0.1662, 0.1758, 0.0752, 0.1740,   -inf,   -inf],\n",
              "         [0.7759, 0.3496, 0.9653, 0.2836,   -inf,   -inf],\n",
              "         [0.5883, 0.2423, 0.0327, 0.0624,   -inf,   -inf]]])"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#AttentionMask with multiple heads\n",
        "rand_attn = torch.rand(1,2,6,6) # 2 heads\n",
        "attention_mask = torch.tensor([1,1,1,1,0,0]).unsqueeze(0).bool()\n",
        "attention_mask = attention_mask.unsqueeze(1).unsqueeze(1)\n",
        "rand_attn.masked_fill_(~attention_mask, float('-inf'))\n",
        "rand_attn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UcgsmWuGlDAR",
        "outputId": "88b77016-1efc-4a77-f9a9-e227fae1aa04"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[[0.6221, 0.7785, 0.0073, 0.2319,   -inf,   -inf],\n",
              "          [0.4934, 0.8452, 0.5697, 0.3594,   -inf,   -inf],\n",
              "          [0.5603, 0.9668, 0.7311, 0.3309,   -inf,   -inf],\n",
              "          [0.2627, 0.3846, 0.4629, 0.7572,   -inf,   -inf],\n",
              "          [0.4675, 0.1218, 0.2573, 0.0620,   -inf,   -inf],\n",
              "          [0.6462, 0.5877, 0.7196, 0.2832,   -inf,   -inf]],\n",
              "\n",
              "         [[0.0208, 0.5180, 0.7303, 0.0207,   -inf,   -inf],\n",
              "          [0.9449, 0.7401, 0.2446, 0.3210,   -inf,   -inf],\n",
              "          [0.3413, 0.3449, 0.3984, 0.3886,   -inf,   -inf],\n",
              "          [0.6146, 0.9930, 0.6880, 0.9037,   -inf,   -inf],\n",
              "          [0.2537, 0.0947, 0.4935, 0.3183,   -inf,   -inf],\n",
              "          [0.4476, 0.2766, 0.8942, 0.6780,   -inf,   -inf]]]])"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Multihead self attention with Attention mask\n",
        "class SelfAttention(nn.Module):\n",
        "  def __init__(self, embed_dim, n_heads, attn_p =0, proj_p = 0):\n",
        "    super().__init__()\n",
        "    self.embed_dim = embed_dim\n",
        "    self.n_heads = n_heads\n",
        "    self.head_dim = self.embed_dim // self.n_heads\n",
        "\n",
        "    self.q_proj = nn.Linear(self.embed_dim, self.embed_dim)\n",
        "    self.k_proj = nn.Linear(self.embed_dim, self.embed_dim)\n",
        "    self.v_proj = nn.Linear(self.embed_dim, self.embed_dim)\n",
        "    self.attn_drop = nn.Dropout(attn_p)\n",
        "    self.proj = nn.Linear(self.embed_dim, self.embed_dim)\n",
        "    self.proj_drop = nn.Dropout(proj_p)\n",
        "  def forward(self, x, attention_mask=None):\n",
        "    batch_size, seq_len , _ = x.shape\n",
        "    print(x.shape)\n",
        "    q = self.q_proj(x).reshape(batch_size, seq_len, self.n_heads, self.head_dim).transpose(1,2).contiguous()\n",
        "    k = self.k_proj(x).reshape(batch_size, seq_len, self.n_heads, self.head_dim).transpose(1,2).contiguous()\n",
        "    v = self.v_proj(x).reshape(batch_size, seq_len, self.n_heads, self.head_dim).transpose(1,2).contiguous()\n",
        "\n",
        "    attn = (q@k.transpose(-2,-1))/(self.head_dim)**0.5\n",
        "\n",
        "    # apply attn mask\n",
        "    if attention_mask is not None:\n",
        "      attention_mask = attention_mask.unsqueeze(1).unsqueeze(1).repeat(1,1,seq_len,1)\n",
        "      attn = attn.masked_fill_(~attention_mask, float(\"-inf\"))\n",
        "    attn = attn.softmax(dim = -1)\n",
        "    attn = self.attn_drop(attn)\n",
        "\n",
        "    print(\"after attention mask\")\n",
        "    print(attn)\n",
        "    x = attn@v\n",
        "\n",
        "    x = x.transpose(1,2).reshape(batch_size, seq_len, embed_dim)\n",
        "    x = self.proj(x)\n",
        "    x = self.proj_drop(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "seq_lens = [3,5,4]\n",
        "embed_dim = 9\n",
        "n_heads = 3\n",
        "model = SelfAttention(embed_dim = embed_dim, n_heads= n_heads)\n",
        "rand = torch.randn(len(seq_lens), max(seq_lens), embed_dim)\n",
        "masks = torch.nn.utils.rnn.pad_sequence([torch.ones(l) for l in seq_lens], batch_first=True, padding_value=0).bool()\n",
        "print(\"attention masks\")\n",
        "print(masks)\n",
        "\n",
        "output = model(rand, attention_mask = masks)\n",
        "print(\"output\")\n",
        "output\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "6HEkLeqRl1hQ",
        "outputId": "578e2bdd-d777-425f-a52a-9067207ff92c"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "attention masks\n",
            "tensor([[ True,  True,  True, False, False],\n",
            "        [ True,  True,  True,  True,  True],\n",
            "        [ True,  True,  True,  True, False]])\n",
            "torch.Size([3, 5, 9])\n",
            "after attention mask\n",
            "tensor([[[[0.4269, 0.1933, 0.3798, 0.0000, 0.0000],\n",
            "          [0.3186, 0.3029, 0.3785, 0.0000, 0.0000],\n",
            "          [0.3290, 0.2973, 0.3737, 0.0000, 0.0000],\n",
            "          [0.2805, 0.4264, 0.2931, 0.0000, 0.0000],\n",
            "          [0.2617, 0.4004, 0.3379, 0.0000, 0.0000]],\n",
            "\n",
            "         [[0.4584, 0.3048, 0.2368, 0.0000, 0.0000],\n",
            "          [0.4060, 0.2677, 0.3263, 0.0000, 0.0000],\n",
            "          [0.2874, 0.2983, 0.4144, 0.0000, 0.0000],\n",
            "          [0.3014, 0.3234, 0.3752, 0.0000, 0.0000],\n",
            "          [0.1804, 0.3904, 0.4292, 0.0000, 0.0000]],\n",
            "\n",
            "         [[0.3517, 0.3001, 0.3482, 0.0000, 0.0000],\n",
            "          [0.3506, 0.4296, 0.2197, 0.0000, 0.0000],\n",
            "          [0.2882, 0.4222, 0.2897, 0.0000, 0.0000],\n",
            "          [0.2676, 0.3445, 0.3879, 0.0000, 0.0000],\n",
            "          [0.2867, 0.3108, 0.4025, 0.0000, 0.0000]]],\n",
            "\n",
            "\n",
            "        [[[0.1903, 0.1837, 0.1810, 0.2100, 0.2350],\n",
            "          [0.1798, 0.0934, 0.1768, 0.2953, 0.2547],\n",
            "          [0.1918, 0.2341, 0.1819, 0.1610, 0.2312],\n",
            "          [0.1945, 0.2363, 0.1879, 0.1736, 0.2076],\n",
            "          [0.2043, 0.1557, 0.2091, 0.1812, 0.2497]],\n",
            "\n",
            "         [[0.2028, 0.2348, 0.1732, 0.1339, 0.2553],\n",
            "          [0.1848, 0.2803, 0.1533, 0.1527, 0.2289],\n",
            "          [0.1820, 0.2838, 0.1291, 0.1473, 0.2579],\n",
            "          [0.1956, 0.2273, 0.1956, 0.1813, 0.2002],\n",
            "          [0.1958, 0.2525, 0.1701, 0.1429, 0.2387]],\n",
            "\n",
            "         [[0.2099, 0.2101, 0.2110, 0.1470, 0.2219],\n",
            "          [0.2456, 0.1577, 0.2104, 0.1650, 0.2213],\n",
            "          [0.1666, 0.2082, 0.2348, 0.2094, 0.1810],\n",
            "          [0.2285, 0.2059, 0.2048, 0.1209, 0.2400],\n",
            "          [0.2542, 0.1698, 0.2054, 0.1308, 0.2399]]],\n",
            "\n",
            "\n",
            "        [[[0.2964, 0.2595, 0.2137, 0.2303, 0.0000],\n",
            "          [0.2111, 0.3112, 0.1788, 0.2988, 0.0000],\n",
            "          [0.2669, 0.2281, 0.2917, 0.2133, 0.0000],\n",
            "          [0.2516, 0.2591, 0.2442, 0.2450, 0.0000],\n",
            "          [0.2337, 0.3071, 0.1956, 0.2636, 0.0000]],\n",
            "\n",
            "         [[0.4398, 0.1873, 0.2761, 0.0968, 0.0000],\n",
            "          [0.0469, 0.4414, 0.1503, 0.3615, 0.0000],\n",
            "          [0.1822, 0.3567, 0.1743, 0.2868, 0.0000],\n",
            "          [0.1459, 0.3444, 0.2456, 0.2642, 0.0000],\n",
            "          [0.0801, 0.5142, 0.1606, 0.2451, 0.0000]],\n",
            "\n",
            "         [[0.2684, 0.1364, 0.3857, 0.2094, 0.0000],\n",
            "          [0.2475, 0.1514, 0.3932, 0.2079, 0.0000],\n",
            "          [0.2076, 0.2811, 0.2139, 0.2974, 0.0000],\n",
            "          [0.2470, 0.2072, 0.3144, 0.2314, 0.0000],\n",
            "          [0.1946, 0.2506, 0.2896, 0.2651, 0.0000]]]],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "output\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[-0.1722,  0.0553, -0.0037, -0.1829,  0.3531, -0.3346, -0.2964,\n",
              "          -0.1518,  0.4501],\n",
              "         [-0.1951,  0.0696, -0.0111, -0.1379,  0.3413, -0.3316, -0.3649,\n",
              "          -0.1019,  0.4501],\n",
              "         [-0.2399,  0.0629, -0.0296, -0.0949,  0.3154, -0.3835, -0.3497,\n",
              "          -0.0767,  0.4696],\n",
              "         [-0.1884,  0.0990, -0.0041, -0.0467,  0.3144, -0.3690, -0.3527,\n",
              "          -0.0743,  0.4343],\n",
              "         [-0.2473,  0.1059, -0.0093, -0.0032,  0.2889, -0.3937, -0.3526,\n",
              "          -0.0518,  0.4668]],\n",
              "\n",
              "        [[-0.2290,  0.0654, -0.0831,  0.1258,  0.2146, -0.1069, -0.3625,\n",
              "          -0.0582,  0.2011],\n",
              "         [-0.2255,  0.0356, -0.1181,  0.1294,  0.2026, -0.1413, -0.3250,\n",
              "          -0.0618,  0.1754],\n",
              "         [-0.2160,  0.0779, -0.0677,  0.1187,  0.2340, -0.0836, -0.3726,\n",
              "          -0.0511,  0.1991],\n",
              "         [-0.2069,  0.0738, -0.0673,  0.1254,  0.2103, -0.1016, -0.3911,\n",
              "          -0.0597,  0.2029],\n",
              "         [-0.2136,  0.0705, -0.0810,  0.1441,  0.2083, -0.1018, -0.3682,\n",
              "          -0.0530,  0.1850]],\n",
              "\n",
              "        [[-0.3363,  0.0910, -0.0365, -0.0496,  0.4212, -0.4533, -0.4988,\n",
              "           0.3077,  0.6110],\n",
              "         [-0.1080,  0.1310,  0.0644, -0.2539,  0.5369, -0.2567, -0.3744,\n",
              "           0.0547,  0.5050],\n",
              "         [-0.2388,  0.1491,  0.0595, -0.1435,  0.4663, -0.2328, -0.4852,\n",
              "           0.1850,  0.5767],\n",
              "         [-0.1864,  0.1426,  0.0590, -0.1677,  0.4776, -0.2714, -0.4430,\n",
              "           0.1368,  0.5527],\n",
              "         [-0.1334,  0.1359,  0.0698, -0.2685,  0.5678, -0.1806, -0.4026,\n",
              "           0.0904,  0.5039]]], grad_fn=<ViewBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#causal masking\n",
        "seq_len = 8\n",
        "ones = torch.ones((seq_len, seq_len))\n",
        "causal_mask = torch.tril(ones).bool()\n",
        "\n",
        "# apply padding mask too\n",
        "padding_mask= torch.tensor([1,1,1,1,0, 0, 0, 0]).bool()\n",
        "padding_mask = padding_mask.unsqueeze(0).repeat(seq_len,1)\n",
        "\n",
        "causal_mask = causal_mask.masked_fill_(~padding_mask, 0)\n",
        "causal_mask"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VZyA3ukznko-",
        "outputId": "1fbecf98-7c61-4a14-99b7-8deeefb5bf5f"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ True, False, False, False, False, False, False, False],\n",
              "        [ True,  True, False, False, False, False, False, False],\n",
              "        [ True,  True,  True, False, False, False, False, False],\n",
              "        [ True,  True,  True,  True, False, False, False, False],\n",
              "        [ True,  True,  True,  True, False, False, False, False],\n",
              "        [ True,  True,  True,  True, False, False, False, False],\n",
              "        [ True,  True,  True,  True, False, False, False, False],\n",
              "        [ True,  True,  True,  True, False, False, False, False]])"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Multihead self attention with Attention mask\n",
        "class SelfAttention(nn.Module):\n",
        "  def __init__(self, embed_dim, n_heads, attn_p =0, proj_p = 0, causal = False):\n",
        "    super().__init__()\n",
        "    self.embed_dim = embed_dim\n",
        "    self.n_heads = n_heads\n",
        "    self.head_dim = self.embed_dim // self.n_heads\n",
        "    self.causal = causal\n",
        "\n",
        "    self.q_proj = nn.Linear(self.embed_dim, self.embed_dim)\n",
        "    self.k_proj = nn.Linear(self.embed_dim, self.embed_dim)\n",
        "    self.v_proj = nn.Linear(self.embed_dim, self.embed_dim)\n",
        "    self.attn_drop = nn.Dropout(attn_p)\n",
        "    self.proj = nn.Linear(self.embed_dim, self.embed_dim)\n",
        "    self.proj_drop = nn.Dropout(proj_p)\n",
        "  def forward(self, x, attention_mask=None):\n",
        "    batch_size, seq_len , _ = x.shape\n",
        "    print(x.shape)\n",
        "    q = self.q_proj(x).reshape(batch_size, seq_len, self.n_heads, self.head_dim).transpose(1,2).contiguous()\n",
        "    k = self.k_proj(x).reshape(batch_size, seq_len, self.n_heads, self.head_dim).transpose(1,2).contiguous()\n",
        "    v = self.v_proj(x).reshape(batch_size, seq_len, self.n_heads, self.head_dim).transpose(1,2).contiguous()\n",
        "\n",
        "    attn = (q@k.transpose(-2,-1))/(self.head_dim)**0.5\n",
        "\n",
        "    if self.causal:\n",
        "      #create a causal mask\n",
        "      ones = torch.ones((seq_len, seq_len), device = attn.device)\n",
        "      causal_mask = torch.tril(ones)\n",
        "\n",
        "      # add dimension for batch and n_heads\n",
        "      causal_mask = causal_mask.reshape(1,1,seq_len,seq_len).bool()\n",
        "\n",
        "\n",
        "      # apply attn mask\n",
        "      if attention_mask is not None:\n",
        "        causal_mask = causal_mask.repeat(batch_size,1,1,1)\n",
        "        attention_mask = attention_mask.unsqueeze(1).unsqueeze(1).repeat(1,1,seq_len,1)\n",
        "        causal_mask.masked_fill_(~attention_mask, False)\n",
        "      attn = attn.masked_fill_(~causal_mask, float(\"-inf\"))\n",
        "    attn = attn.softmax(dim = -1)\n",
        "    attn = self.attn_drop(attn)\n",
        "\n",
        "    print(\"after attention mask\")\n",
        "    print(attn)\n",
        "    x = attn@v\n",
        "\n",
        "    x = x.transpose(1,2).reshape(batch_size, seq_len, embed_dim)\n",
        "    x = self.proj(x)\n",
        "    x = self.proj_drop(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "seq_lens = [3,5,4]\n",
        "embed_dim = 9\n",
        "n_heads = 3\n",
        "model = SelfAttention(embed_dim = embed_dim, n_heads= n_heads, causal=True)\n",
        "rand = torch.randn(len(seq_lens), max(seq_lens), embed_dim)\n",
        "masks = torch.nn.utils.rnn.pad_sequence([torch.ones(l) for l in seq_lens], batch_first=True, padding_value=0).bool()\n",
        "print(\"attention masks\")\n",
        "print(masks)\n",
        "\n",
        "output = model(rand, attention_mask = masks)\n",
        "print(\"output\")\n",
        "output\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "HDWHv8uftLRY",
        "outputId": "7507b116-55b7-49c8-f2cf-f9ccccabbfce"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "attention masks\n",
            "tensor([[ True,  True,  True, False, False],\n",
            "        [ True,  True,  True,  True,  True],\n",
            "        [ True,  True,  True,  True, False]])\n",
            "torch.Size([3, 5, 9])\n",
            "after attention mask\n",
            "tensor([[[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "          [0.5347, 0.4653, 0.0000, 0.0000, 0.0000],\n",
            "          [0.2565, 0.3754, 0.3682, 0.0000, 0.0000],\n",
            "          [0.1956, 0.5322, 0.2722, 0.0000, 0.0000],\n",
            "          [0.4415, 0.2914, 0.2671, 0.0000, 0.0000]],\n",
            "\n",
            "         [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "          [0.3635, 0.6365, 0.0000, 0.0000, 0.0000],\n",
            "          [0.3391, 0.3604, 0.3004, 0.0000, 0.0000],\n",
            "          [0.4478, 0.4288, 0.1234, 0.0000, 0.0000],\n",
            "          [0.1710, 0.3492, 0.4798, 0.0000, 0.0000]],\n",
            "\n",
            "         [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "          [0.6304, 0.3696, 0.0000, 0.0000, 0.0000],\n",
            "          [0.2523, 0.4797, 0.2680, 0.0000, 0.0000],\n",
            "          [0.3931, 0.3325, 0.2743, 0.0000, 0.0000],\n",
            "          [0.3258, 0.2925, 0.3817, 0.0000, 0.0000]]],\n",
            "\n",
            "\n",
            "        [[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "          [0.5964, 0.4036, 0.0000, 0.0000, 0.0000],\n",
            "          [0.5075, 0.2982, 0.1943, 0.0000, 0.0000],\n",
            "          [0.2572, 0.2796, 0.2728, 0.1904, 0.0000],\n",
            "          [0.3697, 0.2241, 0.0877, 0.1264, 0.1922]],\n",
            "\n",
            "         [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "          [0.6043, 0.3957, 0.0000, 0.0000, 0.0000],\n",
            "          [0.4798, 0.3445, 0.1757, 0.0000, 0.0000],\n",
            "          [0.1766, 0.2102, 0.3173, 0.2959, 0.0000],\n",
            "          [0.2290, 0.1876, 0.1775, 0.1747, 0.2312]],\n",
            "\n",
            "         [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "          [0.3884, 0.6116, 0.0000, 0.0000, 0.0000],\n",
            "          [0.3094, 0.3090, 0.3815, 0.0000, 0.0000],\n",
            "          [0.1430, 0.2226, 0.3298, 0.3046, 0.0000],\n",
            "          [0.1844, 0.1928, 0.1476, 0.1505, 0.3247]]],\n",
            "\n",
            "\n",
            "        [[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "          [0.3549, 0.6451, 0.0000, 0.0000, 0.0000],\n",
            "          [0.2825, 0.4694, 0.2482, 0.0000, 0.0000],\n",
            "          [0.3114, 0.2396, 0.2336, 0.2154, 0.0000],\n",
            "          [0.2759, 0.3011, 0.2038, 0.2192, 0.0000]],\n",
            "\n",
            "         [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "          [0.5921, 0.4079, 0.0000, 0.0000, 0.0000],\n",
            "          [0.3805, 0.2322, 0.3873, 0.0000, 0.0000],\n",
            "          [0.2491, 0.2703, 0.2351, 0.2456, 0.0000],\n",
            "          [0.2402, 0.3011, 0.2318, 0.2268, 0.0000]],\n",
            "\n",
            "         [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "          [0.7014, 0.2986, 0.0000, 0.0000, 0.0000],\n",
            "          [0.1920, 0.4854, 0.3226, 0.0000, 0.0000],\n",
            "          [0.0586, 0.5979, 0.1672, 0.1762, 0.0000],\n",
            "          [0.2047, 0.3474, 0.1913, 0.2566, 0.0000]]]],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "output\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[ 0.1671, -0.0286,  0.1037, -0.0251,  0.4422,  0.0877,  0.2845,\n",
              "          -0.0369, -0.6447],\n",
              "         [ 0.1617, -0.1590, -0.0444,  0.1780,  0.3622, -0.2162,  0.0505,\n",
              "          -0.0452, -0.3431],\n",
              "         [ 0.1507, -0.3053, -0.0203,  0.1416,  0.2164, -0.2129,  0.1077,\n",
              "          -0.0880, -0.3099],\n",
              "         [ 0.1185, -0.2628,  0.0477,  0.1022,  0.3259, -0.1692,  0.1274,\n",
              "          -0.1175, -0.3682],\n",
              "         [ 0.1535, -0.3351,  0.0430,  0.0048,  0.2281, -0.0921,  0.2625,\n",
              "          -0.1350, -0.4605]],\n",
              "\n",
              "        [[ 0.0056,  0.0772, -0.1156,  0.2442, -0.3029, -0.2755, -0.5676,\n",
              "           0.2253,  0.2744],\n",
              "         [ 0.3288, -0.0360, -0.0312,  0.2432, -0.0740, -0.0835, -0.0926,\n",
              "           0.1240, -0.1185],\n",
              "         [ 0.4110, -0.1339,  0.0999,  0.0430, -0.0184,  0.1371,  0.2247,\n",
              "          -0.0389, -0.4166],\n",
              "         [ 0.4597, -0.2973,  0.0865, -0.0749,  0.0102,  0.1314,  0.4172,\n",
              "          -0.2244, -0.6192],\n",
              "         [ 0.3527, -0.3146, -0.0435,  0.1315, -0.0435, -0.1370,  0.1150,\n",
              "          -0.0936, -0.2617]],\n",
              "\n",
              "        [[ 0.0242, -0.2720,  0.1167, -0.4287, -0.0541,  0.0935,  0.1694,\n",
              "          -0.3946, -0.5332],\n",
              "         [-0.1504, -0.0850, -0.0410, -0.2313,  0.0995, -0.1591, -0.2017,\n",
              "          -0.3184, -0.3476],\n",
              "         [ 0.0655, -0.2347, -0.1850, -0.0579, -0.1393, -0.2609, -0.2242,\n",
              "          -0.2656, -0.2043],\n",
              "         [ 0.0945, -0.2414, -0.1383,  0.0326, -0.1602, -0.2261, -0.1904,\n",
              "          -0.1421, -0.1415],\n",
              "         [ 0.0374, -0.2418, -0.0657, -0.0332, -0.0942, -0.1594, -0.1348,\n",
              "          -0.1875, -0.2191]]], grad_fn=<ViewBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#cross attention\n",
        "#Multihead self attention with Attention mask\n",
        "class CrossAttention(nn.Module):\n",
        "  def __init__(self, embed_dim, n_heads, attn_p =0, proj_p = 0):\n",
        "    super().__init__()\n",
        "    self.embed_dim = embed_dim\n",
        "    self.n_heads = n_heads\n",
        "    self.head_dim = self.embed_dim // self.n_heads\n",
        "\n",
        "    self.q_proj = nn.Linear(self.embed_dim, self.embed_dim)\n",
        "    self.k_proj = nn.Linear(self.embed_dim, self.embed_dim)\n",
        "    self.v_proj = nn.Linear(self.embed_dim, self.embed_dim)\n",
        "    self.attn_drop = nn.Dropout(attn_p)\n",
        "    self.proj = nn.Linear(self.embed_dim, self.embed_dim)\n",
        "    self.proj_drop = nn.Dropout(proj_p)\n",
        "  def forward(self, src,tgt, attention_mask=None):\n",
        "    batch_size, src_seq_len , _ = src.shape\n",
        "    _,tgt_seq_len,_ = tgt.shape\n",
        "    q = self.q_proj(tgt).reshape(batch_size, tgt_seq_len, self.n_heads, self.head_dim).transpose(1,2).contiguous()\n",
        "    k = self.k_proj(src).reshape(batch_size, src_seq_len, self.n_heads, self.head_dim).transpose(1,2).contiguous()\n",
        "    v = self.v_proj(src).reshape(batch_size, src_seq_len, self.n_heads, self.head_dim).transpose(1,2).contiguous()\n",
        "\n",
        "    attn = (q@k.transpose(-2,-1))/(self.head_dim)**0.5\n",
        "\n",
        "    # apply attn mask\n",
        "    if attention_mask is not None:\n",
        "      attention_mask = attention_mask.unsqueeze(1).unsqueeze(1).repeat(1,1,tgt_seq_len,1)\n",
        "      attn = attn.masked_fill_(~attention_mask, float(\"-inf\"))\n",
        "    attn = attn.softmax(dim = -1)\n",
        "    attn = self.attn_drop(attn)\n",
        "\n",
        "    print(\"after attention mask\")\n",
        "    print(attn)\n",
        "    x = attn@v\n",
        "\n",
        "    x = x.transpose(1,2).reshape(batch_size, tgt_seq_len, embed_dim)\n",
        "    x = self.proj(x)\n",
        "    x = self.proj_drop(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "english_seq_lens = [3,5,4]\n",
        "french_seq_lens = [7,6,2]\n",
        "\n",
        "embed_dim = 18\n",
        "num_heads = 3\n",
        "a = CrossAttention(embed_dim, num_heads)\n",
        "\n",
        "### Create random tensor in the shape (Batch x Seq Len x Embed Dim) for French and English ###\n",
        "### This will be a tensor upto the max(seq_lens) ###\n",
        "rand_english = torch.randn(len(english_seq_lens),max(english_seq_lens),embed_dim)\n",
        "rand_french = torch.randn(len(french_seq_lens),max(french_seq_lens),embed_dim)\n",
        "\n",
        "\n",
        "### Create Attention Mask from the seq_lens (shortest sequences padded to the longest ###\n",
        "english_masks = torch.nn.utils.rnn.pad_sequence([torch.ones(l) for l in english_seq_lens], batch_first=True, padding_value=0).bool()\n",
        "french_masks = torch.nn.utils.rnn.pad_sequence([torch.ones(l) for l in french_seq_lens], batch_first=True, padding_value=0).bool()\n",
        "\n",
        "print(\"English Attention Mask:\")\n",
        "print(english_masks)\n",
        "print(\"French Attention Mask:\")\n",
        "print(french_masks)\n",
        "\n",
        "### Pass through MHA ###\n",
        "output = a(src=rand_english, tgt=rand_french, attention_mask=english_masks)\n",
        "print(\"Final Output:\", output.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "0MfX303mvMhO",
        "outputId": "23b7fd25-1a06-46de-da7d-474cb2275db6"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "English Attention Mask:\n",
            "tensor([[ True,  True,  True, False, False],\n",
            "        [ True,  True,  True,  True,  True],\n",
            "        [ True,  True,  True,  True, False]])\n",
            "French Attention Mask:\n",
            "tensor([[ True,  True,  True,  True,  True,  True,  True],\n",
            "        [ True,  True,  True,  True,  True,  True, False],\n",
            "        [ True,  True, False, False, False, False, False]])\n",
            "after attention mask\n",
            "tensor([[[[0.4222, 0.2891, 0.2887, 0.0000, 0.0000],\n",
            "          [0.4217, 0.2769, 0.3015, 0.0000, 0.0000],\n",
            "          [0.2570, 0.4412, 0.3018, 0.0000, 0.0000],\n",
            "          [0.2927, 0.3663, 0.3410, 0.0000, 0.0000],\n",
            "          [0.1743, 0.5946, 0.2311, 0.0000, 0.0000],\n",
            "          [0.2537, 0.4107, 0.3356, 0.0000, 0.0000],\n",
            "          [0.4089, 0.2630, 0.3281, 0.0000, 0.0000]],\n",
            "\n",
            "         [[0.2534, 0.4615, 0.2850, 0.0000, 0.0000],\n",
            "          [0.4160, 0.2547, 0.3293, 0.0000, 0.0000],\n",
            "          [0.3078, 0.3459, 0.3463, 0.0000, 0.0000],\n",
            "          [0.3992, 0.2319, 0.3689, 0.0000, 0.0000],\n",
            "          [0.3385, 0.3016, 0.3599, 0.0000, 0.0000],\n",
            "          [0.2580, 0.3759, 0.3662, 0.0000, 0.0000],\n",
            "          [0.4269, 0.2218, 0.3513, 0.0000, 0.0000]],\n",
            "\n",
            "         [[0.4404, 0.3208, 0.2388, 0.0000, 0.0000],\n",
            "          [0.1801, 0.2786, 0.5413, 0.0000, 0.0000],\n",
            "          [0.4126, 0.3339, 0.2535, 0.0000, 0.0000],\n",
            "          [0.3228, 0.3580, 0.3191, 0.0000, 0.0000],\n",
            "          [0.4468, 0.2952, 0.2580, 0.0000, 0.0000],\n",
            "          [0.1490, 0.6009, 0.2501, 0.0000, 0.0000],\n",
            "          [0.2323, 0.2468, 0.5208, 0.0000, 0.0000]]],\n",
            "\n",
            "\n",
            "        [[[0.2075, 0.1890, 0.2018, 0.2014, 0.2002],\n",
            "          [0.2562, 0.1632, 0.1495, 0.2139, 0.2171],\n",
            "          [0.1950, 0.1935, 0.1821, 0.2321, 0.1973],\n",
            "          [0.2405, 0.1512, 0.2513, 0.1667, 0.1904],\n",
            "          [0.1991, 0.1917, 0.1767, 0.2333, 0.1992],\n",
            "          [0.2183, 0.1978, 0.1526, 0.2137, 0.2176],\n",
            "          [0.1862, 0.2040, 0.3015, 0.1417, 0.1666]],\n",
            "\n",
            "         [[0.1703, 0.0886, 0.3227, 0.1374, 0.2811],\n",
            "          [0.1700, 0.1893, 0.1498, 0.2470, 0.2438],\n",
            "          [0.1744, 0.1353, 0.2549, 0.1697, 0.2657],\n",
            "          [0.0701, 0.2464, 0.3151, 0.2006, 0.1677],\n",
            "          [0.2740, 0.1649, 0.1589, 0.1789, 0.2233],\n",
            "          [0.2466, 0.1776, 0.1976, 0.1799, 0.1983],\n",
            "          [0.2245, 0.2666, 0.1449, 0.2118, 0.1521]],\n",
            "\n",
            "         [[0.1829, 0.1468, 0.3565, 0.1325, 0.1813],\n",
            "          [0.1504, 0.2202, 0.0871, 0.3532, 0.1891],\n",
            "          [0.1950, 0.2072, 0.1621, 0.2156, 0.2201],\n",
            "          [0.2018, 0.1735, 0.1449, 0.2739, 0.2059],\n",
            "          [0.1985, 0.1726, 0.2368, 0.1724, 0.2197],\n",
            "          [0.1957, 0.2197, 0.0995, 0.2827, 0.2024],\n",
            "          [0.1519, 0.2414, 0.1576, 0.3055, 0.1436]]],\n",
            "\n",
            "\n",
            "        [[[0.2246, 0.2763, 0.2672, 0.2320, 0.0000],\n",
            "          [0.1347, 0.4023, 0.1191, 0.3438, 0.0000],\n",
            "          [0.2510, 0.2523, 0.2753, 0.2215, 0.0000],\n",
            "          [0.2278, 0.2765, 0.2907, 0.2050, 0.0000],\n",
            "          [0.2919, 0.1927, 0.3373, 0.1781, 0.0000],\n",
            "          [0.3040, 0.1532, 0.3626, 0.1802, 0.0000],\n",
            "          [0.1976, 0.3633, 0.1920, 0.2471, 0.0000]],\n",
            "\n",
            "         [[0.1763, 0.2273, 0.1601, 0.4362, 0.0000],\n",
            "          [0.2494, 0.2157, 0.2743, 0.2606, 0.0000],\n",
            "          [0.1968, 0.2318, 0.2256, 0.3458, 0.0000],\n",
            "          [0.2448, 0.1944, 0.3070, 0.2539, 0.0000],\n",
            "          [0.2692, 0.1799, 0.3419, 0.2090, 0.0000],\n",
            "          [0.2690, 0.1230, 0.4195, 0.1884, 0.0000],\n",
            "          [0.2636, 0.2203, 0.2928, 0.2233, 0.0000]],\n",
            "\n",
            "         [[0.2380, 0.2887, 0.2268, 0.2464, 0.0000],\n",
            "          [0.2580, 0.1954, 0.2710, 0.2755, 0.0000],\n",
            "          [0.2555, 0.2452, 0.2528, 0.2465, 0.0000],\n",
            "          [0.2434, 0.1649, 0.3596, 0.2320, 0.0000],\n",
            "          [0.2848, 0.2419, 0.2578, 0.2155, 0.0000],\n",
            "          [0.2447, 0.2884, 0.2326, 0.2344, 0.0000],\n",
            "          [0.2624, 0.2057, 0.2960, 0.2358, 0.0000]]]],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "Final Output: torch.Size([3, 7, 18])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jSYa83Ji-XHS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}